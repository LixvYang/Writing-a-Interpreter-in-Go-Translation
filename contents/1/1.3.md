# 1.3词法分析器
在我们开始coding之前，让我们理清一下本部分的目标。我们将写下我们自己的词法分析器，它将**吃掉**源代码作为输入并且输出它认识的Token。

它不需要缓冲区或保存Tokens，因为只有一个被称为`NextToken()`的方法，它可以输出下一个Token。

这意味着我们将用我们的源代码初始化词法分析器，然后重复调用NextToken()来遍历源代码，一个标记一个标记，一个字符，一个字符。我们还将通过使用字符串作为我们源代码的类型来简化这里的工作。注意：在生产环境中，将文件名和行号附加到Token上是有意义的，它可以更好地追钟语法分析和解析错误。所以最好用`io.Reader`初始化语法分析器和文件名。但是因为这会增加更多的复杂性，我们不会在这里处理，我们将从小开始，只是用字符串并忽略文件名和行号。

考虑到这一点，我们现在意识到我们的词法分析器需要做的很清楚。所以让我们创建一个新package并添加第一个测试，我们可以连续运行该测试以获取有关词法分析器工作状态的反馈。我们从这里开始，以扩展测试用例，为词法分析器添加更多功能：
```go
// lexer/lexer_test.go
package lexer

import (
    "testing"
    "monkey/token"
)

func TestNextToken(t *testing.T){
    input := `=+(){},;`

    tests := []struct{
        expectedType    token.TokenType
        expectedLiteral string
    }{
        {token.ASSIGN,"="},
        {token.PLUS,"+"},
        {token.LPAREN,"("},
        {token.RPAREN,")"},
        {token.LBRACE,"{"},
        {token.RBRACE,"}"},
        {token.COMMA,","},
        {token.SEMICOLON,";"},
        {token.EOF,""},
    }

    l := New(input)
    for i, tt := range tests {
		tok := l.NextToken()

		if tok.Type != tt.expectedType {
			t.Fatalf("tests[%d] - tokentype wrong. expected=%q, got=%q",
				i, tt.expectedType, tok.Type)
		}

		if tok.Literal != tt.expectedLiteral {
			t.Fatalf("tests[%d] - literal wrong. expected=%q, got=%q",
				i, tt.expectedLiteral, tok.Literal)
		}
	}
}
```
当然，这个测试失败了——我们还没有写任何代码:
```
$ go test ./lexer
# monkey/lexer
lexer/lexer_test.go:27: undefined: New
FAIL    monkey/lexer [build f
```

所以让我们通过定义返回*Lexer的New()函数。
```go
// lexer/lexer.go
package lexer

import "monkey/token"

type Lexer struct {
	input        string
	position     int  // current position in input (points to current char)
	readPosition int  // current reading position in input (after current char)
	ch           byte // current char under examination
}

func New(input string) *Lexer {
	l := &Lexer{input: input}
	return l
}
```

许多在Lexer中的字段是不言自明的。有一些可能会引起困扰的是`position`和`readPosition`.这两个将用于它们作为索引来访问输入的字符。例如`l.input[l.readPosition]`。这两个指针指向我们的输入字符串的原因是我们需要能够进一步“窥视”输入并查看当前字符以查看接下来会发送什么。`readPosition`总是指向输入中的“下一个”字。`positions`指向输入中对应于ch的字符。

第一个名为`readChar()`的辅助方法应该使这些字段更容易理解：
```go
//lexer/lexer.go
func (l *Lexer) readChar() {
	if l.readPosition >= len(l.input) {
		l.ch = 0
	} else {
		l.ch = l.input[l.readPosition]
	}
	l.position = l.readPosition
	l.readPosition += 1
}
```

readChar的目的是为我们提供下一个字符并提高我们在输入字符串中的位置。它做的第一件事是检查我们是否到达输入的末尾。如果是，它就设置l.ch为0，这是"NUL"字符的ASCII码，对我们来说表示“我们还没有读取任何东西”或“文件结束”。但是如果我们还没有到达输入的结尾，它会通过访问l.input[l.readPosition]来设置l.ch到下一个字符。

在l.position之后被更新为l.readPosition并且l.readPosition+1。这样，l.readPosition总是指向我们要从next开始读取的下一个位置，而l.position总是指向我们上次读取的位置。这很快就会派上用场。

在谈论readChar时指出，词法分析器仅支持ASCII字符而不是完成的Unicode范围。为什么？因为这让我们保持简单并专注于我们的解释器的基本部分。为了完全支持Unicode和UTF-8，我们需要将l.ch从一个字节更改为rune并更改我们读取下一个字符的方式，因为它们现在可能是多个字节宽。使用l.input[l.readPosition]将不再起作用。然后我们还需要更改一些我们稍后会看到的其他方法和函数。因此在Monkey中完全支持Unicode（和表情符号）作为练习留给读者。

让我们在New函数中使用readChar以便我们的*Lexer在任何人调用NextToken()之前处于完全工作的状态，并且l.ch,l.position和l.readPosition也以及初始化。

```go
//lexer/lexer.go
func New(input string) *Lexer {
    l := &Lexer{input:input}
    l.readChar()
    return l
}
```

我们的测试现在告诉我们，调用New(输入)不会遇到问题，但仍然缺少NextToken()方法。让我们通过添加第一个版本来解决：
```go
//lexer/lexer.go
import "monkey/token"

func (l *Lexer) NextToken() token.Token {
    var tok token.Token

    switch l.ch {
    case'=':
        tok = newToken(token.ASSIGN, l.ch)
    case ';':
		tok = newToken(token.SEMICOLON, l.ch)
    case '(':
		tok = newToken(token.LPAREN, l.ch)
	case ')':
		tok = newToken(token.RPAREN, l.ch)
    case ',':
		tok = newToken(token.COMMA, l.ch)
    case '+':
		tok = newToken(token.PLUS, l.ch)
    case '{':
		tok = newToken(token.LBRACE, l.ch)
	case '}':
		tok = newToken(token.RBRACE, l.ch)
    case 0:
		tok.Literal = ""
		tok.Type = token.EOF
    }
    
    l.readChar()
    return tok
}

func newToken(tokenType token.TokenType, ch byte) token.Token {
	return token.Token{Type: tokenType, Literal: string(ch)}
}
```
